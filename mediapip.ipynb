{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n",
    "# !pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\n",
      "  Using cached imutils-0.5.4-py3-none-any.whl\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EYE_CASCADE_PATH = 'haarcascade\\\\eye.xml'\n",
    "MOUTH_CASCADE_PATH = 'haarcascade\\\\smile.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class of mediapipe face mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mediapip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before starting landmark extraction by mediapip, first was checked if two eyes and mouth exist in the face (by dlib for face detection and haarcascade for eye and mouth classification) or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mediapip-landmark indices\n",
    "RIGHT_EYE_POINTS = [130, 243, 23, 27, 33]\n",
    "LEFT_EYE_POINTS = [463, 257, 359, 253, 263]\n",
    "MOUTH_POINTS = [57, 287, 12, 13 , 16, 17, 314, 268, 315, 61, 291]\n",
    "UP_FACE = 337\n",
    "DOWN_FACE = 396\n",
    "NOSE_POINT = 19\n",
    "FOREHEAD_TH = 338\n",
    "\n",
    "\n",
    "# Points used to detect forehead and chain\n",
    "HELP_POINTS = MOUTH_POINTS + RIGHT_EYE_POINTS + LEFT_EYE_POINTS\n",
    "\n",
    "# used for pose\n",
    "NOSE_POINT_POSE = [1]\n",
    "CHIN_POINT_POSE = [199]\n",
    "MOUTH_POINT_POSE = [61, 291]\n",
    "EYE_POINT_POSE = [33, 263]\n",
    "HELP_POINTS_POSE = NOSE_POINT_POSE + CHIN_POINT_POSE + MOUTH_POINT_POSE + EYE_POINT_POSE\n",
    "\n",
    "FORHEAD_RATIO = 0.3\n",
    "\n",
    "# Used for shadow detection\n",
    "RIGHT_DOWN_EYE = 23\n",
    "LEFT_DOWN_EYE = 253\n",
    "LEFT_MOUTH = 291\n",
    "RIGHT_MOUTH = 61\n",
    "EYE_UP = 336\n",
    "UP_RIGHT = 108\n",
    "UP_LEFT = 333\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model of landmark extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceMeshModel():\n",
    "    def __init__(self, image, min_detection_confidence=0.5, model_selection=0):\n",
    "        self.min_detection_confidence = min_detection_confidence\n",
    "        self.model_selection = model_selection\n",
    "        \n",
    "        # face detection\n",
    "        self.mp_face_detection= mp.solutions.face_detection\n",
    "        self.face_detection = self.mp_face_detection.FaceDetection(min_detection_confidence=0.5, model_selection=0)\n",
    "        \n",
    "        # face mesh\n",
    "        self.face_mesh_lib = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.face_mesh_lib.FaceMesh()\n",
    "        \n",
    "        self.current_landmarks = None\n",
    "        \n",
    "        self.height, self.width = image.shape[:2]\n",
    "        self.results = None\n",
    "        \n",
    "    def show_landmark(self, frame, v_stream=False):\n",
    "        landmarks = self.current_landmarks\n",
    "        width = self.width\n",
    "        height = self.height\n",
    "        for landmark in landmarks[:, :2]:\n",
    "            x = int(landmark[0, 0] * width)\n",
    "            y = int(landmark[0, 1] * height)\n",
    "            cv2.circle(frame, (x,y), radius=4, color=(0,0,200), thickness=-1)\n",
    "        cv2.imshow('landmarks',cv2.resize(frame, (512, 512)))\n",
    "        if v_stream:\n",
    "            cv2.waitKey(1)\n",
    "        else:\n",
    "            cv2.waitKey(0) \n",
    "        \n",
    "        \n",
    "    # retrieve landmarks by mediapip \n",
    "    def get_landmarks(self, start, frame):\n",
    "        out = None\n",
    "        if start:\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # get width and height\n",
    "            self.height, self.width, ch = frame.shape\n",
    "            height, width = self.height, self.width\n",
    "            re = self.face_mesh.process(rgb_frame)\n",
    "            re_landmark = re.multi_face_landmarks[0].landmark\n",
    "            out = np.matrix([[int(landmark.x * width), int(landmark.y * height), landmark.z] for landmark in re_landmark])\n",
    "        return out\n",
    "\n",
    "    # Task one\n",
    "    def check_sensivity(self, im, show):\n",
    "        \n",
    "        out = {'eye': False, 'mouth': False, 'forehead': None, 'chin': None}\n",
    "        \n",
    "        self.current_landmarks = self.get_landmarks(True, im)\n",
    "        landmarks = np.int16(self.current_landmarks)\n",
    "        x_start, y_start = np.min(landmarks[:,0]), np.min(landmarks[:,1])\n",
    "        x_end, y_end = np.max(landmarks[:,0]), np.max(landmarks[:,1])\n",
    "        \n",
    "        detect_eye_mouth = False\n",
    "        detect_forehead = False\n",
    "        detect_chin = False\n",
    "        \n",
    "        height, width = im.shape[:2]\n",
    "        # threshold for box of face\n",
    "        th = 0.1\n",
    "        x_start, y_start = int((max(0, x_start) - th)) , int((max(0, y_start) - th))\n",
    "        x_end, y_end = int((min(x_end, width) + th)), int((min(y_end, height) + th))\n",
    "        # gray level of image to input haarcascade classifier\n",
    "        gr_im = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
    "        # haar cascase detector for eye\n",
    "        eye_detector = cv2.CascadeClassifier(EYE_CASCADE_PATH)    \n",
    "        eyeRects = eye_detector.detectMultiScale(\n",
    "            gr_im[y_start:y_end, x_start:x_end], scaleFactor=1.1, minNeighbors=10,\n",
    "            minSize=(15, 15), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        \n",
    "        # haar cascase detector for mouth\n",
    "        mouth_detector = cv2.CascadeClassifier(MOUTH_CASCADE_PATH)\n",
    "        mouthRects = mouth_detector.detectMultiScale(\n",
    "            gr_im[y_start:y_end, x_start:x_end], scaleFactor=1.1, minNeighbors=10,\n",
    "            minSize=(15, 15), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        # if eye and mouth exist in face without occulsion => extract 468 landmarks\n",
    "        out['eye'] = len(eyeRects) >= 2\n",
    "        out['mouth'] = len(mouthRects) >= 1\n",
    "        if len(eyeRects) >= 2 and len(mouthRects) >= 1:\n",
    "            detect_eye_mouth = True\n",
    "        \n",
    "            \n",
    "        # detect forehead and chain\n",
    "        if detect_eye_mouth:\n",
    "\n",
    "            \n",
    "            help_landmarks = landmarks[HELP_POINTS, :]\n",
    "            x_face_start, y_face_start = np.min(help_landmarks[:,0]), np.min(help_landmarks[:,1])\n",
    "            x_face_end, y_face_end = np.max(help_landmarks[:,0]), np.max(help_landmarks[:,1])\n",
    "            h_eye_mouth = y_face_end - y_face_start\n",
    "            # h_forehead = int(FORHEAD_RATIO*(h_eye_mouth))\n",
    "            right_eye_inner = np.max(landmarks[RIGHT_EYE_POINTS, 0])\n",
    "            left_eye_inner = np.min(landmarks[LEFT_EYE_POINTS, 0])\n",
    "            # to cover part of hair in forehead, we use 5 instead of 4 in below equation\n",
    "            h_forehead = int(4*(left_eye_inner - right_eye_inner)/3)\n",
    "            w_forehead = np.max(help_landmarks[:,0]) - np.min(help_landmarks[:,0])\n",
    "            y_forehead_start = y_face_start-h_forehead\n",
    "            if y_forehead_start >= 0:\n",
    "                detect_forehead = True\n",
    "            # cv2.circle(im, (x_face_start,max(0, y_forehead_start)), radius=4, color=(0,0,200), thickness=-1)\n",
    "            # cv2.circle(im, (x_face_end, y_face_start), radius=4, color=(0,0,200), thickness=-1)\n",
    "            if not detect_forehead:\n",
    "                if landmarks[FOREHEAD_TH, 1] >=0:\n",
    "                    detect_forehead = True\n",
    "            \n",
    "            # chin detection\n",
    "            if height >= landmarks[DOWN_FACE,1]:\n",
    "                detect_chin = True\n",
    "            \n",
    "            out['forehead'], out['chin'] = detect_forehead, detect_chin\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def estimate_head_pose(self, img):\n",
    "        \n",
    "        height, width = self.height, self.width\n",
    "\n",
    "        face2d = []\n",
    "        face3d = []\n",
    "        landmarks = self.current_landmarks\n",
    "        \n",
    "        face2d = [[int(landmark[0, 0]), int(landmark[0, 1])] for landmark in landmarks[HELP_POINTS_POSE,:2]]\n",
    "        face3d = [[int(landmark[0, 0]), int(landmark[0, 1]), landmark[0, 2]] for landmark in landmarks[HELP_POINTS_POSE,:]]\n",
    "        focal_len = 1 * width\n",
    "        \n",
    "        cam_parameter = np.array([ [focal_len, 0, height / 2],\n",
    "                                    [0, focal_len, width / 2],\n",
    "                                    [0, 0, 1]])\n",
    "        face2d = np.array(face2d, dtype=np.float64)\n",
    "        face3d = np.array(face3d, dtype=np.float64)\n",
    "        dist_coff = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "        success, rot_vec, trans_vec = cv2.solvePnP(face3d, face2d, cam_parameter, dist_coff)\n",
    "\n",
    "        rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "        angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "        x = angles[0] * 360\n",
    "        y = angles[1] * 360\n",
    "        z = angles[2] * 360\n",
    "        \n",
    "        return x, y, z\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # Task 2: check appropraite condition\n",
    "    def check_correct_condition(self, im):\n",
    "        distance_flag = False\n",
    "        forward_flag = False\n",
    "        no_shadow_flag = True\n",
    "        # estimate distance to camera\n",
    "        if self.current_landmarks is not None:\n",
    "            z_dist = self.current_landmarks[NOSE_POINT, 2]\n",
    "            if -0.15 <= z_dist <= -0.12: \n",
    "                distance_flag = True\n",
    "            \n",
    "            # pose estimation\n",
    "            pose = self.estimate_head_pose(im)\n",
    "            roll, pitch, yaw = pose[0], pose[1], pose[2]\n",
    "            if -6<roll<6 and -6<pitch<6 and -6<yaw<6:\n",
    "                forward_flag = True\n",
    "            no_shadow_flag = self.detect_shadow(im)\n",
    "            \n",
    "                \n",
    "            # shadow existance\n",
    "            \n",
    "            \n",
    "        return {'near': distance_flag, 'forward': forward_flag, 'no shadow': no_shadow_flag}\n",
    "                \n",
    "                \n",
    "                \n",
    "    # Task 3: check shadow on face\n",
    "    def detect_shadow(self, im):\n",
    "        no_shadow_flag = True\n",
    "        org_im = np.zeros((self.height, self.width))\n",
    "        \n",
    "        # To get boundry of face\n",
    "        landmarks = np.int16(self.current_landmarks)\n",
    "        Xmin = max(0, np.min(landmarks[:, 0]))\n",
    "        Ymin = max(0, np.min(landmarks[:, 1]))\n",
    "        Xmax = max(0, np.max(landmarks[:, 0]))\n",
    "        Ymax = max(0, np.max(landmarks[:, 1]))\n",
    "        \n",
    "        im = im[Ymin: Ymax, Xmin:Xmax]\n",
    "        if len(im):\n",
    "        \n",
    "            gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "            blurred = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "            (T, threshInv) = cv2.threshold(blurred, 150, 255,\n",
    "                cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "            im = threshInv\n",
    "            \n",
    "            org_im[Ymin: Ymax, Xmin:Xmax] = im\n",
    "            im = org_im\n",
    "    #        cv2.imshow('face', im)\n",
    "     #       cv2.waitKey(0)\n",
    "            \n",
    "            # right cheek coordinates\n",
    "            x1_r_cheek = int(self.current_landmarks[132,0])\n",
    "            y1_r_cheek = int(self.current_landmarks[RIGHT_DOWN_EYE,1])\n",
    "            x2_r_cheek = int(self.current_landmarks[RIGHT_MOUTH, 0])\n",
    "            y2_r_cheek = int(self.current_landmarks[RIGHT_MOUTH, 1])\n",
    "            right_cheek = im[y1_r_cheek: y2_r_cheek, x1_r_cheek: x2_r_cheek]\n",
    "            # cv2.rectangle(im, (x1_r_cheek, y1_r_cheek), (x2_r_cheek, y2_r_cheek), (50, 150, 50), 5)\n",
    "            \n",
    "            # left cheek coordinates\n",
    "            x1_l_cheek = int(self.current_landmarks[LEFT_MOUTH, 0])\n",
    "            y1_l_cheek = int(self.current_landmarks[LEFT_DOWN_EYE,1])\n",
    "            x2_l_cheek = int(self.current_landmarks[352, 0])\n",
    "            y2_l_cheek = int(self.current_landmarks[LEFT_MOUTH, 1])\n",
    "            left_cheek = im[y1_l_cheek: y2_l_cheek, x1_l_cheek: x2_l_cheek]\n",
    "            # cv2.rectangle(im, (x1_l_cheek, y1_l_cheek), (x2_l_cheek, y2_l_cheek), (50, 150, 50), 5)\n",
    "            \n",
    "            # forehead coordinates\n",
    "            x1_fh = int(np.min((self.current_landmarks[:, 0])))\n",
    "            y1_fh = int(np.min(self.current_landmarks[:, 1]))\n",
    "            x2_fh = int(np.max((self.current_landmarks[:, 0])))\n",
    "            y2_fh = int(self.current_landmarks[EYE_UP, 1])\n",
    "            forehead = im[y1_fh: y2_fh, x1_fh: x2_fh]\n",
    "            # cv2.rectangle(im, (x1_fh, y1_fh), (x2_fh, y2_fh), (50, 150, 100), 5)\n",
    "            \n",
    "            skin = np.append(right_cheek, left_cheek)\n",
    "            skin = np.append(forehead, skin)\n",
    "            \n",
    "            area = skin.shape[0]\n",
    "            white = 100 * len(skin[skin==255]) / area\n",
    "            black = 100 * len(skin[skin==0]) / area\n",
    "            \n",
    "            # Check uniform distribution of illumination\n",
    "            if min(white, black) >= 20:\n",
    "                no_shadow_flag = False\n",
    "            \n",
    "        return no_shadow_flag   \n",
    "\n",
    "\n",
    "\n",
    "    def extract_landmarks(self, frame, show=False, v_stream=False):\n",
    "        landmarks = None\n",
    "        se_flag = True\n",
    "        one_face_flag = True\n",
    "        co_flag = True\n",
    "        \n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # extract faces\n",
    "        self.results = self.face_detection.process(rgb_frame).detections\n",
    "        if self.results:\n",
    "            if len(self.results) == 1:\n",
    "                sensivity_cond = self.check_sensivity(frame, show)\n",
    "                print(sensivity_cond)\n",
    "                if False in sensivity_cond.values():\n",
    "                    se_flag = False\n",
    "                correct_cond = self.check_correct_condition(frame)\n",
    "                print(correct_cond)\n",
    "                if False in correct_cond.values():\n",
    "                    co_flag = False\n",
    "            else:\n",
    "                one_face_flag = False\n",
    "                \n",
    "            if se_flag and one_face_flag and co_flag: \n",
    "                if show:\n",
    "                    self.show_landmark(frame, v_stream)\n",
    "                return self.current_landmarks\n",
    "            else:\n",
    "                if show:\n",
    "                    cv2.imshow('frame', frame)\n",
    "                    if v_stream:\n",
    "                        cv2.waitKey(1)\n",
    "                    else:\n",
    "                        cv2.waitKey(0)\n",
    "                \n",
    "        # no face or multiple faces\n",
    "        return landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eye': True, 'mouth': True, 'forehead': True, 'chin': True}\n",
      "{'near': False, 'forward': True, 'no shadow': True}\n"
     ]
    }
   ],
   "source": [
    "i = cv2.imread('images\\\\test2.jpg')\n",
    "extractor_landmark = FaceMeshModel(i)\n",
    "\n",
    "extractor_landmark.extract_landmarks(i, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "v_stream = True\n",
    "\n",
    "while True:\n",
    "    ret, f = cap.read()\n",
    "    if ret:\n",
    "        extractor_landmark = FaceMeshModel(f)\n",
    "        extractor_landmark.extract_landmarks(f, show=True, v_stream=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4eaf1be304415beee96765ae99c3f893cc8312c7f1196698e6029668e9aeb3e5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
